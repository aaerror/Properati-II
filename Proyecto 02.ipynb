{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Proyecto-02:-Ingeniería-de-features,-Modelos-avanzados-e-Interpretación-de-modelos\" data-toc-modified-id=\"Proyecto-02:-Ingeniería-de-features,-Modelos-avanzados-e-Interpretación-de-modelos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><strong>Proyecto 02</strong>: Ingeniería de features, Modelos avanzados e Interpretación de modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#INICIO\" data-toc-modified-id=\"INICIO-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><strong>INICIO</strong></a></span></li><li><span><a href=\"#Análisis-previo\" data-toc-modified-id=\"Análisis-previo-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><strong>Análisis previo</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Análisis-de-las-distintas-variables\" data-toc-modified-id=\"Análisis-de-las-distintas-variables-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Análisis de las distintas variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Análisis-de-variables-numéricas-DISCRETAS\" data-toc-modified-id=\"Análisis-de-variables-numéricas-DISCRETAS-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Análisis de variables numéricas <strong>DISCRETAS</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Ambientes,-Baños-y-Habitaciones\" data-toc-modified-id=\"Ambientes,-Baños-y-Habitaciones-1.2.1.1.1\"><span class=\"toc-item-num\">1.2.1.1.1&nbsp;&nbsp;</span>Ambientes, Baños y Habitaciones</a></span></li></ul></li><li><span><a href=\"#Análisis-de-variables-numéricas-CONTINUAS\" data-toc-modified-id=\"Análisis-de-variables-numéricas-CONTINUAS-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Análisis de variables numéricas <strong>CONTINUAS</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Sup.-cubierta,-Sup.-total-y-Precio\" data-toc-modified-id=\"Sup.-cubierta,-Sup.-total-y-Precio-1.2.1.2.1\"><span class=\"toc-item-num\">1.2.1.2.1&nbsp;&nbsp;</span>Sup. cubierta, Sup. total y Precio</a></span></li></ul></li><li><span><a href=\"#Análisis-de-la-variables-CATEGÓRICAS\" data-toc-modified-id=\"Análisis-de-la-variables-CATEGÓRICAS-1.2.1.3\"><span class=\"toc-item-num\">1.2.1.3&nbsp;&nbsp;</span>Análisis de la variables <strong>CATEGÓRICAS</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Tipo-de-propiedad\" data-toc-modified-id=\"Tipo-de-propiedad-1.2.1.3.1\"><span class=\"toc-item-num\">1.2.1.3.1&nbsp;&nbsp;</span>Tipo de propiedad</a></span></li><li><span><a href=\"#Barrios\" data-toc-modified-id=\"Barrios-1.2.1.3.2\"><span class=\"toc-item-num\">1.2.1.3.2&nbsp;&nbsp;</span>Barrios</a></span></li></ul></li></ul></li><li><span><a href=\"#Extra\" data-toc-modified-id=\"Extra-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Extra</a></span></li><li><span><a href=\"#Últimos-ajustes-del-dataset\" data-toc-modified-id=\"Últimos-ajustes-del-dataset-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Últimos ajustes del dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sup.-total->=-Sup.-cubierta\" data-toc-modified-id=\"Sup.-total->=-Sup.-cubierta-1.2.3.1\"><span class=\"toc-item-num\">1.2.3.1&nbsp;&nbsp;</span>Sup. total &gt;= Sup. cubierta</a></span></li><li><span><a href=\"#Ambientes->=-Habitaciones\" data-toc-modified-id=\"Ambientes->=-Habitaciones-1.2.3.2\"><span class=\"toc-item-num\">1.2.3.2&nbsp;&nbsp;</span>Ambientes &gt;= Habitaciones</a></span></li><li><span><a href=\"#Aplicando-filtros\" data-toc-modified-id=\"Aplicando-filtros-1.2.3.3\"><span class=\"toc-item-num\">1.2.3.3&nbsp;&nbsp;</span>Aplicando filtros</a></span></li></ul></li><li><span><a href=\"#Correlaciones\" data-toc-modified-id=\"Correlaciones-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Correlaciones</a></span></li></ul></li><li><span><a href=\"#PARTE-A:-Transformación-de-Datos\" data-toc-modified-id=\"PARTE-A:-Transformación-de-Datos-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><strong>PARTE A: Transformación de Datos</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesamiento-de-datos\" data-toc-modified-id=\"Preprocesamiento-de-datos-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Preprocesamiento de datos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding\" data-toc-modified-id=\"Encoding-1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>Encoding</a></span></li><li><span><a href=\"#Imputación-de-valores-faltantes\" data-toc-modified-id=\"Imputación-de-valores-faltantes-1.3.1.2\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>Imputación de valores faltantes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Análisis-de-valores-nulos\" data-toc-modified-id=\"Análisis-de-valores-nulos-1.3.1.2.1\"><span class=\"toc-item-num\">1.3.1.2.1&nbsp;&nbsp;</span>Análisis de valores nulos</a></span></li><li><span><a href=\"#Imputaciones\" data-toc-modified-id=\"Imputaciones-1.3.1.2.2\"><span class=\"toc-item-num\">1.3.1.2.2&nbsp;&nbsp;</span>Imputaciones</a></span></li></ul></li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-1.3.1.3\"><span class=\"toc-item-num\">1.3.1.3&nbsp;&nbsp;</span>Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Datos-clasificados-como-outliers\" data-toc-modified-id=\"Datos-clasificados-como-outliers-1.3.1.3.1\"><span class=\"toc-item-num\">1.3.1.3.1&nbsp;&nbsp;</span>Datos clasificados como outliers</a></span></li><li><span><a href=\"#Datos-finales\" data-toc-modified-id=\"Datos-finales-1.3.1.3.2\"><span class=\"toc-item-num\">1.3.1.3.2&nbsp;&nbsp;</span>Datos finales</a></span></li><li><span><a href=\"#Extra\" data-toc-modified-id=\"Extra-1.3.1.3.3\"><span class=\"toc-item-num\">1.3.1.3.3&nbsp;&nbsp;</span>Extra</a></span></li></ul></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-1.3.1.4\"><span class=\"toc-item-num\">1.3.1.4&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#DBSCAN\" data-toc-modified-id=\"DBSCAN-1.3.1.4.1\"><span class=\"toc-item-num\">1.3.1.4.1&nbsp;&nbsp;</span>DBSCAN</a></span></li><li><span><a href=\"#K-MEANS\" data-toc-modified-id=\"K-MEANS-1.3.1.4.2\"><span class=\"toc-item-num\">1.3.1.4.2&nbsp;&nbsp;</span>K-MEANS</a></span></li></ul></li><li><span><a href=\"#Escalado-de-datos\" data-toc-modified-id=\"Escalado-de-datos-1.3.1.5\"><span class=\"toc-item-num\">1.3.1.5&nbsp;&nbsp;</span>Escalado de datos</a></span></li><li><span><a href=\"#Reducción-de-dimensionalidad\" data-toc-modified-id=\"Reducción-de-dimensionalidad-1.3.1.6\"><span class=\"toc-item-num\">1.3.1.6&nbsp;&nbsp;</span>Reducción de dimensionalidad</a></span></li></ul></li><li><span><a href=\"#Modelo-del-Proyecto-01\" data-toc-modified-id=\"Modelo-del-Proyecto-01-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Modelo del Proyecto 01</a></span><ul class=\"toc-item\"><li><span><a href=\"#DecisionTreeRegressor\" data-toc-modified-id=\"DecisionTreeRegressor-1.3.2.1\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span><strong>DecisionTreeRegressor</strong></a></span></li></ul></li></ul></li><li><span><a href=\"#PARTE-B:-Modelos-avanzados\" data-toc-modified-id=\"PARTE-B:-Modelos-avanzados-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><strong>PARTE B: Modelos avanzados</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#LinearRegressor\" data-toc-modified-id=\"LinearRegressor-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>LinearRegressor</a></span></li><li><span><a href=\"#Polynominal-features\" data-toc-modified-id=\"Polynominal-features-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Polynominal features</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Regularización\" data-toc-modified-id=\"Regularización-1.4.2.0.1\"><span class=\"toc-item-num\">1.4.2.0.1&nbsp;&nbsp;</span>Regularización</a></span></li></ul></li><li><span><a href=\"#SVD\" data-toc-modified-id=\"SVD-1.4.2.1\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>SVD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regularización\" data-toc-modified-id=\"Regularización-1.4.2.1.1\"><span class=\"toc-item-num\">1.4.2.1.1&nbsp;&nbsp;</span>Regularización</a></span></li></ul></li></ul></li><li><span><a href=\"#Random-forest\" data-toc-modified-id=\"Random-forest-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Random forest</a></span></li></ul></li><li><span><a href=\"#PARTE-C:-Interpretación-de-los-modelos\" data-toc-modified-id=\"PARTE-C:-Interpretación-de-los-modelos-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span><strong>PARTE C: Interpretación de los modelos</strong></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 02**: Ingeniería de features, Modelos avanzados e Interpretación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **INICIO**\n",
    "\n",
    "Cargamos las librerías que vamos a utilizar y el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos algunos parámetros globales para los gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# RELACIONADOS A AXES\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams[\"axes.linewidth\"] = 2\n",
    "plt.rcParams[\"axes.spines.bottom\"] = True\n",
    "plt.rcParams[\"axes.spines.left\"] = True\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "\n",
    "# RELACIONADOS A X_TICKS\n",
    "plt.rcParams[\"xtick.alignment\"] = \"center\"\n",
    "plt.rcParams[\"xtick.labelsize\"] = 5\n",
    "plt.rcParams[\"xtick.major.pad\"] = 4\n",
    "plt.rcParams[\"xtick.major.size\"] = 6\n",
    "plt.rcParams[\"xtick.major.width\"] = 2\n",
    "# RELACIONADOS A Y_TICKS\n",
    "plt.rcParams[\"ytick.alignment\"] = \"center\"\n",
    "plt.rcParams[\"ytick.labelsize\"] = 5\n",
    "plt.rcParams[\"ytick.major.pad\"] = 4\n",
    "plt.rcParams[\"ytick.major.size\"] = 6\n",
    "plt.rcParams[\"ytick.major.width\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset y renombramos las columnas para una mejor interpretación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati = pd.read_csv(\".\\Proyectos\\Resources\\Properati.csv\")\n",
    "#properati = pd.read_csv(\"..\\Resources\\Properati.csv\")\n",
    "# Cambio los nombres de las columnas\n",
    "properati.rename(columns={ \"start_date\" : \"Fecha inicio\", \"end_date\" : \"Fecha fin\",\n",
    "                           \"created_on\" : \"Fecha creación\", \"lat\" : \"Latitud\", \"lon\" : \"Longitud\",\n",
    "                           \"l1\" : \"País\", \"l2\" : \"Zona\", \"l3\" : \"Barrios\", \"rooms\" : \"Ambientes\",\n",
    "                           \"bedrooms\" : \"Habitaciones\", \"bathrooms\" : \"Baños\",\n",
    "                           \"surface_total\" : \"Sup. total\",\n",
    "                           \"surface_covered\" : \"Sup. cubierta\", \"price\" : \"Precio\",\n",
    "                           \"currency\" : \"Moneda\", \"title\" : \"Título\", \"description\" : \"Descripción\",\n",
    "                           \"property_type\" : \"Tipo de propiedad\",\n",
    "                           \"operation_type\" : \"Tipo de operación\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar a indagar y para realizar una comparación fidedigna, nos centraremos, al igual que en el Proyecto 01, en analizar los registros de la zona **Capital Federal** filtrando los **Departamentos**, **PHs** y las **Casas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casa = \"Casa\"\n",
    "dpto = \"Departamento\"\n",
    "ph = \"PH\"\n",
    "\n",
    "properati.query('(Zona == \"Capital Federal\") &\\\n",
    "                 (`Tipo de propiedad` == @casa | `Tipo de propiedad` == @dpto | `Tipo de propiedad` == @ph)', inplace=True)\n",
    "properati.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, eliminamos algunas columnas innecesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eliminamos las columnas innecesarias\n",
    "properati.drop(columns={ \"Descripción\", \"Fecha inicio\", \"Fecha fin\", \"Fecha creación\", \"Latitud\",\n",
    "                         \"Longitud\", \"Moneda\", \"País\", \"Título\", \"Tipo de operación\", \"Zona\" },\n",
    "               inplace=True)\n",
    "print(f\"Contamos con { properati.shape[0] } registros y { properati.shape[1] } columnas.\")\n",
    "properati.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Análisis previo**\n",
    "\n",
    "Recordando el análisis realizado en el Proyecto 01, utilizamos el método ```describe()``` para conseguir una lectura estadística general antes de adentrarnos en el detalle de cada atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, encontramos un dataset sucio con mucha presencia de outliers en todas las columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de las distintas variables\n",
    "\n",
    "En esta sección analizaremos las distintas variables para poder abordar correctamente el preprocesamiento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIÓN PARA GRAFICAR OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graficar_outliers(columna, data=properati):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20,5), dpi=100)\n",
    "    # BOXPLOT\n",
    "    sns.boxplot(y=columna, data=data, color=\"Coral\", fliersize=2, linewidth=2, saturation=1, ax=axs[0])    \n",
    "    axs[0].set_xlabel(f\"{ columna.upper() }\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "    axs[0].set_ylabel(f\"CANTIDAD\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "    axs[0].tick_params(axis=\"both\", labelsize=15)\n",
    "    \n",
    "    # DISTPLOT\n",
    "    sns.histplot(x=columna, data=data, bins=30, color=\"Royalblue\", stat=\"probability\", ax=axs[1])\n",
    "    axs[1].set_xlabel(f\"{ columna.upper() }\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "    axs[1].set_ylabel(\"PROBABILIDAD\", fontsize=10, labelpad=10, weight=\"bold\")    \n",
    "    axs[1].tick_params(axis=\"both\", labelsize=15)\n",
    "\n",
    "    fig.suptitle(f\"Estudio outliers: { columna }\".upper(), y=1.05, fontsize=15, fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de variables numéricas **DISCRETAS**\n",
    "---\n",
    "##### Ambientes, Baños y Habitaciones\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = properati.query(\"Ambientes < 10\")\n",
    "graficar_outliers(\"Ambientes\", data)\n",
    "\n",
    "data = properati.query(\"Baños < 5\")\n",
    "graficar_outliers(\"Baños\", data)\n",
    "\n",
    "data = properati.query(\"Habitaciones < 10\")\n",
    "graficar_outliers(\"Habitaciones\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de variables numéricas **CONTINUAS**\n",
    "---\n",
    "##### Sup. cubierta, Sup. total y Precio\n",
    "---\n",
    "\n",
    "Antes de analizar la superficie cubierta debemos notar que esta debe ser menor a la superficie total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = properati.query(\"(`Sup. cubierta` <= `Sup. total`) &\\\n",
    "                        (`Sup. cubierta` > 15 & `Sup. cubierta` < 400)\")\n",
    "graficar_outliers(\"Sup. cubierta\", data)\n",
    "\n",
    "data = data.query(\"`Sup. total` < 1000\")\n",
    "graficar_outliers(\"Sup. total\", data)\n",
    "\n",
    "data = properati.query(\"Precio < 1000000\")\n",
    "graficar_outliers(\"Precio\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de la variables **CATEGÓRICAS**\n",
    "\n",
    "---\n",
    "##### Tipo de propiedad\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = properati.pivot_table(index=\"Tipo de propiedad\", values=\"Barrios\", aggfunc=\"count\").sort_values(\"Barrios\", ascending=False)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=table.index, y=\"Barrios\", data=table, color=\"Royalblue\", edgecolor=\"black\", linewidth=2, order=table.index)\n",
    "plt.title(\"Registros de propiedades\".upper(), fontsize=20)\n",
    "plt.xlabel(\"Tipo de propiedad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Cantidad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Barrios\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrio, cantidad = np.unique(properati[\"Barrios\"], return_counts=True)\n",
    "idx = np.argsort(-cantidad)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.bar(x=barrio[idx], height=cantidad[idx], color=\"Coral\", edgecolor=\"white\", linewidth=2)\n",
    "plt.title(f\"Registros de barrios (Capital Federal)\".upper(), fontsize=25)\n",
    "plt.xlabel(\"Barrios\".upper(), fontsize=15, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=20, rotation=90)\n",
    "plt.ylabel(\"Cantidad\".upper(), fontsize=15, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "Como un apartado extra en el análisis de los datos y sus relaciones, vamos a ver como se comporta el precio en relación al tipo de propiedades y a los barrios. De este modo, podemos comprobar si el barrio tiene influencia en el precio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = properati[\"Precio\"].groupby(properati[\"Tipo de propiedad\"]).mean().round(2).reset_index(name=\"Precio promedio\")\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=\"Tipo de propiedad\", y=\"Precio promedio\", data=table, color=\"Royalblue\",\n",
    "            edgecolor=\"black\", linewidth=2)\n",
    "plt.title(\"Precio promedio\".upper(), fontsize=20)\n",
    "plt.xlabel(\"Tipo de propiedad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Precio promedio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el precio de las ```Casas``` es mayor al de ```Departamento``` y ```PH```. Ahora vamos a analizar los precios de acuerdo al barrio en donde se encuentra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "table = properati.pivot_table(index=\"Barrios\", columns=[\"Tipo de propiedad\"], values=\"Precio\",\n",
    "                              aggfunc=\"mean\").round(2).sort_values(\"Casa\", ascending=False)\n",
    "table.plot(ax=ax, kind=\"bar\")\n",
    "plt.title(\"Precio promedio según propiedad\".upper(), fontsize=20)\n",
    "plt.xlabel(\"BARRIOS\", fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"PRECIO PROMEDIO\", fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(fontsize=20, fancybox=True, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = properati[\"Sup. total\"].groupby(properati[\"Tipo de propiedad\"], sort=True).mean().round(2).reset_index(name=\"Sup. total promedio\")\n",
    "propiedades = table[\"Tipo de propiedad\"]\n",
    "idx = np.argsort(-table[\"Sup. total promedio\"])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=propiedades[idx], y=\"Sup. total promedio\", data=table, color=\"Royalblue\",\n",
    "            edgecolor=\"black\", linewidth=2)\n",
    "plt.title(f\"Superficie total promedio\".upper(), fontsize=20)\n",
    "plt.xlabel(\"Tipo de propiedad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Sup. total promedio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati[\"Sup. total\"].groupby(properati[\"Tipo de propiedad\"], sort=True).mean().round(2).reset_index(name=\"Precio promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Últimos ajustes del dataset\n",
    "\n",
    "Antes de verificar las correlaciones, debemos establecer algunos filtros en el dataset:\n",
    "* La superficie cubierta no puede ser mayor a la superficie total.\n",
    "* La cant. de ambientes no puede ser inferior a la cant. de habitaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Sup. total >= Sup. cubierta\n",
    "\n",
    "Realizamos el filtrado y verificamos los valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "properati.query(\"`Sup. total` >= `Sup. cubierta`\").isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Si nosotros realizamos establecemos que la ```Sup. total >= Sup. cubierta``` eliminamos una proporción del dataset ya que hay comparaciones en la que involucramos valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "properati.query(\"`Sup. total`.isnull() | `Sup. cubierta`.isnull()\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Existen 9.434 registros que involucran valores nulos, ya sea en ```Sup. total``` como en ```Sup. cubierta```. En caso de aplicar el filtro, estamos eliminando un 10.31% del dataset.\n",
    "\n",
    "Si a estos valores nulos los tomamos en cuenta en nuestro dataset obtenemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "properati.query(\"(`Sup. total` >= `Sup. cubierta`) | (`Sup. total`.isnull() | `Sup. cubierta`.isnull())\",\n",
    "                engine=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Existen solo 760 registros que no se adecuan al filtro que realizamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Ambientes >= Habitaciones\n",
    "\n",
    "Realizamos el filtrado y verificamos los valores nulos como a su vez el tamaño de registros involucrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "registros = properati.query(\"Ambientes < Habitaciones\").shape[0]\n",
    "print(\"Cant. de registros:\", registros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "properati.query(\"Ambientes < Habitaciones\").isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como podemos observar, existen solo 63 registros que no cumplen este filtro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando filtros\n",
    "\n",
    "Aplicamos los filtros correspondientes y continuamos a la siguiente sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati = properati.query(\"(`Sup. total` >= `Sup. cubierta`) | (`Sup. total`.isnull() | `Sup. cubierta`.isnull())\",                                engine=\"python\")\n",
    "properati = properati.query(\"Ambientes >= Habitaciones\")\n",
    "properati = properati.reindex([\"Barrios\", \"Ambientes\", \"Baños\", \"Habitaciones\",\n",
    "                               \"Sup. cubierta\", \"Sup. total\", \"Precio\", \"Tipo de propiedad\" ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habiendo aplicado los filtros correspondientes, previamente verificamos el tamaño del dataset con el que vamos a trabajar y los valores nulos que sortear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column = \"\"\n",
    "for i in properati.columns:\n",
    "    if properati[i].isna().sum() > 0:\n",
    "        #isna_count = properati[i].isna().sum()\n",
    "        cantidad = properati[i].isna().sum()\n",
    "        total = properati[i].count()\n",
    "        porcentaje = round(100*cantidad/total, 2)\n",
    "        column += f\"\\t{ i } tiene { properati[i].isna().sum() } datos nulos ({ porcentaje }%).\\n\"\n",
    "print(\"Cant. de registros:\", properati.shape[0])\n",
    "print(\"Columnas con valores nulos:\")\n",
    "print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlaciones\n",
    "\n",
    "Gráficamos un heatmap para encontrar visualizar las correlaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIÓN PARA GRAFICAR CORRELACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlaciones(data, start, end, type='pearson'):\n",
    "    correlaciones = data.iloc[:,start:end].corr(type)\n",
    "    mask = np.zeros(correlaciones.shape, dtype=bool)\n",
    "    mask[np.triu_indices(len(mask))] = True\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    ax = sns.heatmap(correlaciones, annot=True, annot_kws={ \"fontsize\": \"15\", \"fontweight\": \"bold\" },                                cbar_kws={ \"shrink\" : .5 }, cmap=\"coolwarm\", linewidths=2, mask=mask, square=True,\n",
    "                     vmax=1,vmin=-1)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=15, length=0)\n",
    "    plt.title(f\"Correlación { type }\".upper(), fontsize=20, horizontalalignment=\"center\")\n",
    "    plt.tick_params(length=0)\n",
    "    plt.xticks(fontsize=15, horizontalalignment=\"center\", weight=\"bold\")\n",
    "    plt.yticks(fontsize=15, verticalalignment=\"center\", weight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlaciones(properati, 1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE A: Transformación de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos\n",
    "\n",
    "Transformamos las variables a valores categóricos y enteros en caso de corresponder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "data = properati.copy()\n",
    "data[\"Barrios\"] = data[\"Barrios\"].astype(\"category\")\n",
    "data[\"Ambientes\"] = data[\"Ambientes\"].astype(\"int32\")\n",
    "data[\"Habitaciones\"] = data[\"Habitaciones\"].astype(\"int32\")\n",
    "data[\"Tipo de propiedad\"] = data[\"Tipo de propiedad\"].astype(\"category\")\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "\n",
    "Comenzamos el preprocesamiento con la codificación de la variable categórica ```Tipo de propiedad```. Para realizar dicha tarea, hacemos uso del módulo de ```sklearn.preprocessing``` donde encontramos la clase ```LabelEncoder```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_propiedades = data[\"Tipo de propiedad\"]\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Tipo de propiedad\"] = label_encoder.fit_transform(data[\"Tipo de propiedad\"].values)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo módulo, ```sklearn.preprocessing```, utilizamos la clase ```OneHotEncoder``` para evitar que el modelo de machine learning interprete ordinalidad (0<1<2) en la codificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoding = one_hot_encoder.fit_transform(data[[\"Tipo de propiedad\"]])\n",
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.DataFrame(encoding, columns=[\"Casa\", \"Dpto\", \"PH\"])\n",
    "## Reseteamos el index del dataframe\n",
    "data = data.reset_index(drop=True)\n",
    "data = pd.concat([data, columns], axis=1)\n",
    "data[\"Casa\"] = data[\"Casa\"].astype(\"int32\")\n",
    "data[\"Dpto\"] = data[\"Dpto\"].astype(\"int32\")\n",
    "data[\"PH\"] = data[\"PH\"].astype(\"int32\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación de valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Análisis de valores nulos\n",
    "\n",
    "Vamos a analizar que atributos poseen datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como podemos observar, nos encontramos con 2.163 datos faltantes en el atributo ```Baños```, 8.916 en ```Sup. total``` y 9.234 en ```Sup. cubierta```.\n",
    "Pasamos a analizar más en detalle los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "**Baños**\n",
    "\n",
    "---\n",
    "\n",
    "Comenzamos analizando el atributo ```Baños```, encontrando a que tipo de dato nulo corresponde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask = np.isnan(data[\"Baños\"])\n",
    "data[mask].iloc[:,1:7].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**MAR**\n",
    "\n",
    "Pasamos a analizar el atributo ```Baños``` en relación al ```Tipo de propiedad``` para determinar si existe algún tipo de relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.Baños.isnull().groupby(data[\"Tipo de propiedad\"]).sum().astype(int).reset_index(name=\"Valores nulos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Aquí podemos observar que la nulidad afecta principalmente a los departamentos, a su vez, esta es la que mayor cantidad de registros posee. Recordemos el análisis de esta variable previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**MNAR: BAÑOS**\n",
    "\n",
    "Tomamos los datos del dataset estableciendo un filtro en el precio de $3.000.000.\n",
    "\n",
    "Creamos intervalos respecto del precio para verificar cuántos valores nulos corresponden a cada intervalo. Luego, obtenemos la frecuencia y el punto medio del intervalo para realizar un gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mnar = data.query(\"Precio < 3000000\").copy()\n",
    "mnar[\"INTERVALOS\"] = pd.cut(mnar.Precio, bins=25)\n",
    "intervalos = mnar[\"Baños\"].isna().groupby(mnar.INTERVALOS).sum().astype(int).reset_index(name=\"Cantidad\")\n",
    "intervalos[\"FRECUENCIAS\"] = intervalos.Cantidad/mnar[\"Baños\"].groupby([mnar.INTERVALOS]).count().values\n",
    "intervalos[\"BIN CENTRES\"] = intervalos.INTERVALOS.apply(lambda x: x.mid)\n",
    "intervalos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"MNAR: Baños\".upper(), fontsize=15)\n",
    "sns.lineplot(x=\"BIN CENTRES\", y=\"FRECUENCIAS\", data=intervalos, color=\"Royalblue\", linewidth=2)\n",
    "plt.xlabel(\"Punto medio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Frec. de valores faltantes\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Verificamos que el precio influye en la nulidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "**Sup. cubierta y Sup. total**\n",
    "\n",
    "---\n",
    "\n",
    "Pasamos a analizar la superficies y entender a que corresponden la nulidad de sus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask = np.isnan(data[\"Sup. cubierta\"])\n",
    "data[mask].iloc[:,1:7].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask = np.isnan(data[\"Sup. total\"])\n",
    "data[mask].iloc[:,1:7].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Superficie cubierta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"Sup. cubierta\"].isnull().groupby(data[\"Tipo de propiedad\"]).sum().astype(int).reset_index(name=\"Valores nulos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Superficie total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"Sup. total\"].isnull().groupby(data[\"Tipo de propiedad\"]).sum().astype(int).reset_index(name=\"Valores nulos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Se observa, nuevamente, que la nulidad afecta principalmente a los departamentos, a su vez, esta es la que mayor cantidad de registros posee. Esto se replica tanto en la ```Sup. cubierta``` y ```Sup. total```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**MNAR**\n",
    "\n",
    "Realizamos el mismo análisis que hicimos en ```Baños```. Tratamos de entender como se comporta la nulidad de la ```Sup. cubierta``` y ```Sup. total``` respecto al ```Precio```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "**Superficie cubierta**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mnar = data.query(\"Precio < 3000000\").copy()\n",
    "mnar[\"INTERVALOS\"] = pd.cut(mnar[\"Precio\"], bins=25)\n",
    "intervalos = mnar[\"Sup. cubierta\"].isna().groupby(mnar.INTERVALOS).sum().astype(int).reset_index(name=\"Cantidad\")\n",
    "intervalos[\"FRECUENCIAS\"] = intervalos.Cantidad/mnar[\"Sup. cubierta\"].groupby([mnar.INTERVALOS]).count().values\n",
    "intervalos[\"BIN CENTRES\"] = intervalos.INTERVALOS.apply(lambda x: x.mid)\n",
    "intervalos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"MNAR: Superficie cubierta\".upper(), fontsize=20)\n",
    "sns.lineplot(x=\"BIN CENTRES\", y=\"FRECUENCIAS\", data=intervalos, color=\"Royalblue\", linewidth=2)\n",
    "plt.xlabel(\"Punto medio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\", rotation=0)\n",
    "plt.ylabel(\"Frec. de valores faltantes\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "**Superficie total**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mnar = data.query(\"Precio < 3000000\").copy()\n",
    "mnar[\"INTERVALOS\"] = pd.cut(mnar[\"Precio\"], bins=25)\n",
    "intervalos = mnar[\"Sup. total\"].isna().groupby(mnar.INTERVALOS).sum().astype(int).reset_index(name=\"Cantidad\")\n",
    "intervalos[\"FRECUENCIAS\"] = intervalos.Cantidad/mnar[\"Sup. total\"].groupby([mnar.INTERVALOS]).count().values\n",
    "intervalos[\"BIN CENTRES\"] = intervalos.INTERVALOS.apply(lambda x: x.mid)\n",
    "intervalos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"MNAR: Superficie total\".upper(), fontsize=20)\n",
    "sns.lineplot(x=\"BIN CENTRES\", y=\"FRECUENCIAS\", data=intervalos, color=\"Royalblue\", linewidth=2)\n",
    "plt.xlabel(\"Punto medio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Frec. de valores faltantes\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como podemos observar, la nulidad de los datos posee cierta correlación con el precio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputaciones\n",
    "\n",
    "Luego del análisis de los atributos ```Baños```, ```Sup. cubierta```, ```Sup. total```, procedemos a la imputación de valores faltantes.\n",
    "Hacemos uso del módulo experimental ```enable_iterative_imputer```, ```IterativeImputer```.\n",
    "\n",
    "Tomamos los atributos ```Ambientes```, ```Habitaciones``` y las etiquetas que codificamos previamente de la variable categórica ```Tipo de propiedad```: ```Casa```, ```Dpto```, ```PH```. A partir de estos, entrenamos un árbol de decisión y analizamos su puntaje para predecir el atributo de ```Baños```.\n",
    "\n",
    "Utilizamos la clase ```cross_val_score``` para evaluar el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Baños\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"Ambientes\", \"Baños\", \"Habitaciones\", \"Casa\", \"Dpto\", \"PH\"]].dropna()\n",
    "y = data[[\"Baños\"]].dropna()\n",
    "print(\"SCORE:\", cross_val_score(DecisionTreeClassifier(), X, y, cv=5, n_jobs=-1).mean().round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí podemos observar el puntaje que obtuvimos: 99%. Es importante este mismo, ya que al utilizar la clase ```IterativeImputer```, y, en base a estos mismos atributos que seleccionamos, hacemos una imputación de los datos faltantes en la colunna de ```Baños```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_imputer = IterativeImputer(DecisionTreeClassifier(), max_iter=200, max_value=5, min_value=1,\n",
    "                                     random_state=40)\n",
    "# DATOS PARA ENTRENAR EL MODELO\n",
    "fit = data[[\"Ambientes\", \"Baños\", \"Habitaciones\", \"Casa\", \"Dpto\", \"PH\"]]\n",
    "iterative_imputer.fit(fit)\n",
    "\n",
    "# DATOS NULOS A PREDECIR\n",
    "X = data.query(\"Baños.isnull()\", engine=\"python\")\n",
    "X = X[[\"Ambientes\", \"Baños\", \"Habitaciones\", \"Casa\", \"Dpto\", \"PH\"]]\n",
    "impute = iterative_imputer.transform(X)\n",
    "impute = pd.DataFrame(impute, columns=[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Casa\", \"Dpto\", \"PH\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos las imputaciones que se realizaron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_propiedad = [ \"Casa\", \"Dpto\", \"PH\" ]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5), frameon=True, tight_layout=True)\n",
    "for x in range(0,3):\n",
    "    sns.histplot(impute.query(f\"{ tipo_propiedad[x] } == 1\")[\"Baños\"], bins=30, color=\"Royalblue\",\n",
    "                 discrete=True, stat=\"probability\", ax=axs[x])\n",
    "    axs[x].tick_params(axis=\"both\", labelsize=15)\n",
    "    axs[x].set_title(f\"{ tipo_propiedad[x] }\".upper(), fontsize=20)\n",
    "    axs[x].set_xlabel(\"Baños\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[x].set_ylabel(\"Probabilidad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5), frameon=True)\n",
    "plt.title(\"Imputación: Baños\".upper(), fontsize=20)\n",
    "sns.countplot(x=data[\"Baños\"], alpha=.4, color=\"Royalblue\", label=\"Raw data\")\n",
    "sns.countplot(x=impute[\"Baños\"], color=\"Coral\", saturation=1, label=\"Imputados\")\n",
    "plt.xlabel(\"Baños\".upper(), fontsize=10, labelpad=10, weight=\"bold\")\n",
    "plt.xlim(0,4)\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Cantidad\".upper(), fontsize=10, labelpad=10, weight=\"bold\")\n",
    "plt.ylim(0,2000)\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.legend(fontsize=15, fancybox=True, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que se han realizado distintos valores de imputaciones. Procedemos a anexar los mismos a nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de valores nulos\n",
    "idx = data.query(\"Baños.isnull()\", engine=\"python\").index\n",
    "# Serie de valores + ID\n",
    "values = pd.Series(impute[\"Baños\"].values, name=\"Impute\", index=pd.Index(idx, name=\"Index\"))\n",
    "data[\"Baños\"].fillna(values, inplace=True)\n",
    "data[\"Baños\"] = data[\"Baños\"].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Sup. cubierta y Sup. total\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIÓN PARA ANALIZAR LAS IMPUTACIONES DE LA SUP. CUBIERTA Y LA SUP. TOTAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_imputacion(imputados):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(25,10), frameon=True, tight_layout=True)\n",
    "    fig.suptitle(f\"Análisis de imputaciones\".upper(), y=1.05, fontsize=25, fontweight=\"bold\")\n",
    "\n",
    "    sns.kdeplot(x=imputados[\"Sup. cubierta\"], alpha=.8, color=\"Coral\", fill=True, label=\"Imputados\",\n",
    "                linewidth=2, ax=axs[0])\n",
    "    sns.kdeplot(x=data[\"Sup. cubierta\"], alpha=.4, color=\"Royalblue\", fill=True, label=\"Raw data\",\n",
    "                linewidth=4, ax=axs[0])\n",
    "    axs[0].set_xlabel(f\"Sup. cubierta\".upper(), fontsize=15, labelpad=20, weight=\"bold\")\n",
    "    axs[0].set_ylabel(\"Probabilidad\".upper(), fontsize=15, labelpad=20, weight=\"bold\")\n",
    "    axs[0].tick_params(axis=\"both\", labelsize=20)\n",
    "    axs[0].legend(fontsize=20, fancybox=True, frameon=True)\n",
    "    axs[0].set_xlim(0,300)\n",
    "\n",
    "    sns.scatterplot(x=\"Sup. cubierta\", y=\"Sup. total\", alpha=.4, color=\"Royalblue\", data=data,\n",
    "                    label=\"Raw data\", ax=axs[1])\n",
    "    sns.scatterplot(x=\"Sup. cubierta\", y=\"Sup. total\", alpha=.8, color=\"Coral\", data=imputados,\n",
    "                    label=\"Imputados\", ax=axs[1])\n",
    "    axs[1].set_xlabel(\"Sup. cubierta\".upper(), fontsize=15, labelpad=15, weight=\"bold\")\n",
    "    axs[1].set_ylabel(\"Sup. total\".upper(), fontsize=15, labelpad=15, weight=\"bold\")\n",
    "    axs[1].tick_params(axis=\"both\", labelsize=20)\n",
    "    axs[1].legend(fontsize=25, fancybox=True, frameon=True)\n",
    "    axs[1].set_xlim(0,500)\n",
    "    axs[1].set_ylim(0,500)\n",
    "    \n",
    "    sns.kdeplot(x=imputados[\"Sup. total\"], alpha=.8, color=\"Coral\", cbar=True, fill=True, label=\"Imputados\",\n",
    "                linewidth=2, shade=True, ax=axs[2])\n",
    "    sns.kdeplot(x=data[\"Sup. total\"], alpha=.4, color=\"Royalblue\", cbar=True, fill=True, label=\"Raw data\",\n",
    "                linewidth=4, shade=True, ax=axs[2])\n",
    "    axs[2].set_xlabel(f\"Sup. total\".upper(), fontsize=15, labelpad=20, weight=\"bold\")\n",
    "    axs[2].set_ylabel(\"Probabilidad\".upper(), fontsize=15, labelpad=20, weight=\"bold\")\n",
    "    axs[2].tick_params(axis=\"both\", labelsize=20)\n",
    "    axs[2].legend(fontsize=20, fancybox=True, frameon=True)\n",
    "    axs[2].set_xlim(0,300)\n",
    "    plt.show()\n",
    "    \n",
    "def graficar_imputacion_propiedades(imputados):\n",
    "    casas = imputados.query(\"Casa == 1\")\n",
    "    dpto = imputados.query(\"Dpto == 1\")\n",
    "    ph = imputados.query(\"PH == 1\")\n",
    "    \n",
    "    casas_raw = data.query(\"Casa == 1\")\n",
    "    dpto_raw = data.query(\"Dpto == 1\")\n",
    "    ph_raw = data.query(\"PH == 1\")\n",
    "    \n",
    "    dataset = [ casas, dpto, ph ]\n",
    "    dataset_raw = [ casas_raw, dpto_raw, ph_raw ]\n",
    "    propiedades = [ \"Casas\", \"Departamentos\", \"PHs\"]\n",
    "    atributos = [ \"Sup. cubierta\", \"Sup. total\" ]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 3, figsize=(25,15), frameon=True, tight_layout=True)\n",
    "    fig.suptitle(f\"Análisis de imputaciones en las propiedades\".upper(), y=1.05,\n",
    "                 fontsize=25, fontweight=\"bold\")\n",
    "    for y in range(0,2):\n",
    "        for x in range(0,3):\n",
    "            sns.kdeplot(x=dataset[x][atributos[y]], alpha=.8, color=\"Coral\", fill=True,\n",
    "                        label=\"Imputados\", linewidth=2, shade=True, ax=axs[y][x])\n",
    "            sns.kdeplot(x=dataset_raw[x][atributos[y]], alpha=.4, color=\"Royalblue\", fill=True,\n",
    "                        label=\"Raw\", linewidth=4, shade=True, ax=axs[y][x])            \n",
    "            axs[y][x].legend(fontsize=20, fancybox=True, frameon=True)\n",
    "            axs[y][x].tick_params(axis=\"both\", labelsize=20)\n",
    "            axs[y][x].set_title(f\"{ propiedades[x] }\".upper(), fontsize=20)\n",
    "            axs[y][x].set_xlabel(f\"{ atributos[y].upper() }\", fontsize=15, labelpad=20, weight=\"bold\")\n",
    "            axs[y][x].set_xlim(0,500)\n",
    "            axs[y][x].set_ylabel(\"Probabilidad\".upper(), fontsize=15, labelpad=20, weight=\"bold\")            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos previamente en nuestro gráfico de correlaciones, las variables ```Sup. cubierta``` y ```Sup. total``` poseen una alta relación.\n",
    "Hacemos una consulta estableciendo algunos límites para poder quitar el mayor ruido posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = properati.query(\"(`Sup. total` < 1000) & (`Sup. cubierta` > 15 & `Sup. cubierta` < 400)\")\n",
    "\n",
    "plt.figure(figsize=(15,5), frameon=True)\n",
    "sns.scatterplot(x=\"Sup. cubierta\", y=\"Sup. total\", data=query, hue=\"Tipo de propiedad\")\n",
    "plt.title(\"Relación entre Sup. Cubierta y Sup. Total\".upper(), fontsize=15, horizontalalignment=\"center\")\n",
    "plt.xlabel(\"Sup. cubierta\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "#plt.xlim(0,20000)\n",
    "plt.ylabel(\"Sup. total\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "#plt.ylim(0,20000)\n",
    "plt.legend(fontsize=15, fancybox=True, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos filtros, procedemos a entrenar un modelo de regresión utilizando, adicionalmente, los atributos de ```Ambientes```, ```Habitaciones``` y ```Baños``` con los atributos de ```Tipo de propiedad``` que codificamos previamente.\n",
    "\n",
    "Antes de continuar, analizamos los datos que quedan exentos del filtro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exentos = data.query(\"((`Sup. total` > 1000) & (`Sup. cubierta` > 1000)) |\\\n",
    "                      (`Sup. total`.isnull() | `Sup. cubierta`.isnull())\", engine=\"python\")\n",
    "print(\"Cantidad de registros exentos: \", exentos.shape[0])\n",
    "print(\"Cantidad de registros exentos no nulos: \", exentos.dropna().shape[0])\n",
    "exentos.iloc[:,4:6].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen 9.519 exentos del filtro que establecimos pero solo 86 son valores no nulos.\n",
    "\n",
    "Encontramos que la diferencia entre el valor mínimo y máximo es muy grande *—la desviación estandar que obtenemos de la lectura es muy alta—*. También observamos que la desviación estandar es mayor en ```Sup. cubierta```  que en ```Sup. total``` al igual que la media de los respectivos atributos. Este análisis de 9.519 registros nos induce que son, en su mayoría, outliers los cuales contaminan la muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = data.query(\"(`Sup. total` < 1000) & (`Sup. cubierta` > 15 & `Sup. cubierta` < 400)\")\n",
    "X = query[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\",\n",
    "            \"Casa\", \"Dpto\", \"PH\" ]].dropna().reset_index()\n",
    "\n",
    "sup = query[[\"Sup. cubierta\"]].dropna().reset_index()\n",
    "y_cubierta = sup.iloc[:81230,1]\n",
    "\n",
    "sup = query[[\"Sup. total\"]].dropna().reset_index()\n",
    "y_total = sup.iloc[:81230,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos tres modelos y obtenemos el coeficiente de determinación (R<sup>2</sup>) de cada uno para elegir el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score R<sup>2</sup>** para ```Sup. cubierta```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbors = cross_val_score(KNeighborsRegressor(), X, y_cubierta, cv=5, scoring=\"r2\",\n",
    "                             n_jobs=-1).mean().round(6)\n",
    "decision_tree = cross_val_score(DecisionTreeRegressor(), X, y_cubierta, cv=5, scoring=\"r2\",\n",
    "                                n_jobs=-1).mean().round(6)\n",
    "linear_regression = cross_val_score(LinearRegression(), X, y_cubierta, cv=5, scoring=\"r2\",\n",
    "                                    n_jobs=-1).mean().round(6)\n",
    "\n",
    "print(f\"SCORE R2 KNeighborsRegressor: { kneighbors }\")\n",
    "print(f\"SCORE R2 DecisionTreeRegressor: { decision_tree }\")\n",
    "print(f\"SCORE R2 LinearRegression: { linear_regression }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score R<sup>2</sup>** para ```Sup. total```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbors = cross_val_score(KNeighborsRegressor(), X, y_total, cv=5, scoring=\"r2\",\n",
    "                             n_jobs=-1).mean().round(6)\n",
    "decision_tree = cross_val_score(DecisionTreeRegressor(), X, y_total, cv=5, scoring=\"r2\",\n",
    "                                n_jobs=-1).mean().round(6)\n",
    "linear_regression = cross_val_score(LinearRegression(), X, y_total, cv=5, scoring=\"r2\",\n",
    "                                    n_jobs=-1).mean().round(6)\n",
    "\n",
    "print(f\"SCORE R2 KNeighborsRegressor: { kneighbors }\")\n",
    "print(f\"SCORE R2 DecisionTreeRegressor: { decision_tree }\")\n",
    "print(f\"SCORE R2 LinearRegression: { linear_regression }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los valores obtenidos, podemos utilizar el modelo de ```DecisionTreeRegressor``` o el modelo ```LinearRegression```.\n",
    "\n",
    "Extraemos del filtro que analizamos previamente los datos con los que vamos a entrenar un modelo de imputación ```IterativeImputer```. Adicionalmente, entrenamos otro modelo sin utilizar el filtrado que hemos realizado para fines comparativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = data.query(\"(`Sup. total` <= 1000) & (`Sup. cubierta` >= 15 & `Sup. cubierta` <= 400)\")\n",
    "query = query.query(\"Ambientes <= 10 & Habitaciones <= 10 & Baños <= 5\")\n",
    "query = query[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\",\n",
    "                \"Casa\", \"Dpto\", \"PH\" ]]\n",
    "iimputer_filtrado = IterativeImputer(LinearRegression(n_jobs=-1), add_indicator=True,\n",
    "                                     min_value=15, max_iter=200, random_state=40)\n",
    "iimputer_filtrado.fit(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\",\n",
    "           \"Casa\", \"Dpto\", \"PH\" ]]\n",
    "iimputer_raw = IterativeImputer(LinearRegression(n_jobs=-1),\n",
    "                                min_value=15, max_iter=200, random_state=40)\n",
    "iimputer_raw.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos los datos nulos que necesitamos imputar, en este caso corresponden a los atributos de ```Sup. cubierta``` y ```Sup. total```. Luego, realizamos las imputaciones de los datos y analizamos los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_nulos = data.query(\"(`Sup. total`.isnull() | `Sup. cubierta`.isnull())\", engine=\"python\")\n",
    "transform = valores_nulos[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\",\n",
    "                            \"Casa\", \"Dpto\", \"PH\" ]]\n",
    "imputaciones = iimputer_filtrado.fit_transform(transform)\n",
    "filtrado = pd.DataFrame(imputaciones, columns=[ \"Ambientes\", \"Baños\", \"Habitaciones\",\n",
    "                                                \"Sup. cubierta\", \"Sup. total\",\n",
    "                                                \"Casa\", \"Dpto\", \"PH\", \"Miss C\", \"Miss T\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_nulos = data.query(\"(`Sup. total`.isnull() | `Sup. cubierta`.isnull())\", engine=\"python\")\n",
    "transform = valores_nulos[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\",\n",
    "                            \"Casa\", \"Dpto\", \"PH\" ]]\n",
    "impute = iimputer_raw.transform(transform)\n",
    "raw = pd.DataFrame(impute, columns=[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\",\n",
    "                                     \"Sup. total\", \"Casa\", \"Dpto\", \"PH\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_imputacion(raw)\n",
    "graficar_imputacion_propiedades(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_imputacion(filtrado.query(\"`Miss C` == 1 | `Miss T` == 1\"))\n",
    "graficar_imputacion_propiedades(filtrado.query(\"`Miss C` == 1 | `Miss T` == 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a las gráficas que analizamos previamente, utilizamos los datos predichos del modelo sin filtrado ya que tuvieron un mejor desempeño pero antes de hacer las imputaciones, evaluamos que no existan ```Sup. cubierta``` mayores a la ```Sup. total```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.query(\"`Sup. total` < `Sup. cubierta`\").shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputamos aquellos registros que fueron correctamente predichos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.query(\"`Sup. cubierta`.isnull() | `Sup. total`.isnull()\", engine=\"python\").index\n",
    "cubierta = pd.Series(raw[\"Sup. cubierta\"].values, name=\"Impute\", index=pd.Index(idx, name=\"Index\"))\n",
    "total = pd.Series(raw[\"Sup. total\"].values, name=\"Impute\", index=pd.Index(idx, name=\"Index\"))\n",
    "mask = (cubierta.values < total.values)\n",
    "data[\"Sup. cubierta\"].fillna(cubierta[mask], inplace=True)\n",
    "data[\"Sup. total\"].fillna(total[mask], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a entrar un modelo pero en este caso utilizamos un ```DecisionTreeRegressor```. Teniendo en cuenta que solo son registros tipo departamento, entrenamos un modelo con esos datos unicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fit = data.query(\"Dpto == 1\")\n",
    "X = data_fit[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\" ]]\n",
    "iimputer_raw = IterativeImputer(DecisionTreeRegressor(), max_iter=100, n_nearest_features=1,\n",
    "                                random_state=40)\n",
    "iimputer_raw.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_nulos = data.query(\"(`Sup. cubierta`.isnull())\", engine=\"python\")\n",
    "transform = valores_nulos[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\" ]]\n",
    "impute = iimputer_raw.transform(transform)\n",
    "raw = pd.DataFrame(impute, columns=[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\",\n",
    "                                     \"Sup. total\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que las imputaciones respeten la condición previa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.query(\"`Sup. total` < `Sup. cubierta`\").shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos las imputaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.query(\"`Sup. cubierta`.isnull()\", engine=\"python\").index\n",
    "values = pd.Series(raw[\"Sup. cubierta\"].values, name=\"Impute\", index=pd.Index(idx, name=\"Index\"))\n",
    "data[\"Sup. cubierta\"].fillna(values, inplace=True)\n",
    "\n",
    "data.drop(columns=[\"Tipo de propiedad\"], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "Teniendo en cuenta la lectura que hemos hecho en el análisis previo, interpretando el ```describe``` en base a los valores de cota máxima  *—```max```—*, la media *—```mean```—* y la desviación estandar *—```std```—*, junto a las gráficas de cada atributo, podemos inferir que todas las columnas númericas del dataset poseen outliers.\n",
    "\n",
    "Para eliminar los outliers vamos a utilizar el modelo ```IsolationForest``` de la librería ```sklearn.ensemble```. Con este modelo, ajustamos el hiperparámetro ```contamination```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = data.copy()\n",
    "X = outliers.drop(columns=[ \"Barrios\" ])\n",
    "isolation_forest = IsolationForest(n_estimators=700, bootstrap=True, contamination=.1,\n",
    "                                   max_features=.6, max_samples=.6, n_jobs=-1,\n",
    "                                   random_state=42)\n",
    "#isolation_forest = IsolationForest(n_estimators=700, bootstrap=True, contamination=.1,\n",
    "                                   #max_features=.6, max_samples=.8, n_jobs=-1,\n",
    "                                   #random_state=42)\n",
    "out = isolation_forest.fit_predict(X)\n",
    "data[\"Outlier\"] = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficamos para visualizar la cantidad de datos que se han clasificados como outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"Outlier\", y=\"Outlier\", data=data, color=\"Royalblue\", estimator=lambda x:len(x)/len(data)*100,\n",
    "            order=data.groupby('Outlier').count().sort_values(\"Precio\", ascending=False).index)\n",
    "plt.title(\"Proporción de outliers\".upper(), fontsize=20)\n",
    "plt.xlabel(\"Outlier\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks((0,1), (\"Outliers\", \"Data\"), fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Porcentaje\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos un estudio estadistico de la manipulación que hemos hecho con los outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Datos clasificados como outliers\n",
    "\n",
    "Analizamos los datos que hemos clasificados como outliers para comprender que datos estamos eliminando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with_outliers = data.query(\"Outlier == -1\")\n",
    "with_outliers.iloc[:,:7].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Inicialmente, analizamos los datos categóricos que estamos dejando fuera de nuestro dataset final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = data.query(\"Casa == 1 | Dpto == 1 | PH == 1\").groupby(by=[\"Barrios\"], sort=True)\\\n",
    "        .agg(Cantidad=pd.NamedAgg(column=\"Outlier\", aggfunc=\"count\")).reset_index()\n",
    "a.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = with_outliers.query(\"Casa == 1 | Dpto == 1 | PH == 1\").groupby(by=[\"Barrios\"], sort=True)\\\n",
    "                 .agg(Cantidad=pd.NamedAgg(column=\"Outlier\", aggfunc=\"count\")).reset_index()\n",
    "b.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out_barrios = with_outliers.query(\"Casa == 1 | Dpto == 1 | PH == 1\").groupby(by=[\"Barrios\"],\n",
    "                                                                             sort=True)\\\n",
    "              .agg(Casa=pd.NamedAgg(column=\"Casa\", aggfunc=\"sum\"),\n",
    "                   Dpto=pd.NamedAgg(column=\"Dpto\", aggfunc=\"sum\"),\n",
    "                   PH=pd.NamedAgg(column=\"PH\", aggfunc=\"sum\"),\n",
    "                   Cantidad=pd.NamedAgg(column=\"Outlier\", aggfunc=\"count\")).reset_index()\n",
    "percent = round(b[\"Cantidad\"]*100/a[\"Cantidad\"],2)\n",
    "out_barrios[\"Porcentaje\"] = percent\n",
    "out_barrios.sort_values(by=\"Porcentaje\", ascending=False, inplace=True)\n",
    "out_barrios.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out_barrios.plot(figsize=(20,15), x=\"Barrios\", fontsize=15, kind=\"bar\", sort_columns=False,\n",
    "                 subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos finales\n",
    "\n",
    "Aquí analizamos los datos que finalmente van a pertencer al dataset final y con el cual vamos a trabajar luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_outliers = data.query(\"Outlier == 1\")\n",
    "without_outliers.iloc[:,:7].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_outliers(\"Ambientes\", without_outliers)\n",
    "graficar_outliers(\"Baños\", without_outliers)\n",
    "graficar_outliers(\"Habitaciones\", without_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5), constrained_layout=True)\n",
    "plt.title(\"Outliers: Superficie cubierta\".upper(), fontsize=20)\n",
    "mask = data[\"Outlier\"] == 1\n",
    "sns.kdeplot(x=data[mask][\"Sup. cubierta\"], alpha=.8, color=\"Coral\", fill=True, label=\"Without outliers\")\n",
    "mask = data[\"Outlier\"] == -1\n",
    "sns.kdeplot(data[mask][\"Sup. cubierta\"], alpha=.4, color=\"Royalblue\", fill=True, label=\"Raw data\")\n",
    "plt.xlim(0, 300)\n",
    "plt.xlabel(\"Superficie cubierta\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Probabilidad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.legend(fontsize=15, fancybox=True, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5), constrained_layout=True)\n",
    "plt.title(\"Outliers: Superficie total\".upper(), fontsize=15)\n",
    "\n",
    "mask = data[\"Outlier\"] == 1\n",
    "sns.kdeplot(x=data[mask][\"Sup. total\"], alpha=.8, color=\"Coral\", fill=True, label=\"Without outliers\")\n",
    "mask = data[\"Outlier\"] == -1\n",
    "sns.kdeplot(x=data[mask][\"Sup. total\"], alpha=.4, color=\"Royalblue\", fill=True, label=\"Raw data\")\n",
    "plt.xlim(0, 300)\n",
    "plt.xlabel(\"Superficie total\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Probabilidad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.legend(fontsize=15, fancybox=True, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5), constrained_layout=True)\n",
    "plt.title(\"Outliers: Precio\".upper(), fontsize=15)\n",
    "mask = data[\"Outlier\"] == 1\n",
    "sns.kdeplot(data[mask][\"Precio\"], alpha=.8, color=\"Coral\", fill=True, label=\"Without outliers\")\n",
    "mask = data[\"Outlier\"] == -1\n",
    "sns.kdeplot(data[mask][\"Precio\"], alpha=.4, color=\"Royalblue\", fill=True, label=\"Raw data\")\n",
    "plt.xlim(0, 1000000)\n",
    "plt.xlabel(\"Precio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\")\n",
    "plt.ylabel(\"Probabilidad\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.legend(fontsize=15, fancybox=True, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos las correlaciones luego de haber realizado el filtro de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlaciones(without_outliers, 1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los outliers del dataset luego de haber hecho el análisis correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query(\"Outlier == 1\")\n",
    "data.drop(columns=[\"Outlier\"], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos la cantidad de registros (81.596) que conseguimos luego de eliminar los outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Cant. de registros:\", data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra\n",
    "\n",
    "Como apartado extra, luego de haber realizado las imputaciones correspondientes y habiendo eliminado los outliers, creamos una variable para determinar el valor del metro<sup>2</sup> de la propiedad. Con esta misma, podemos obtener una relación entre el precio de las propiedades y el barrio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Precio/mts2\"] = round(data[\"Precio\"]/data[\"Sup. total\"], 2)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una tabla pivot donde obtenemos la media, por barrio, de la variable que creamos previamente (precio/mts<sup>2</sup>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = data.pivot_table(\"Precio/mts2\", \"Barrios\", aggfunc=\"mean\").round(2).reset_index()\n",
    "plt.figure(figsize=(15,6), constrained_layout=True)\n",
    "sns.barplot(x=\"Barrios\", y=\"Precio/mts2\", data=table, color=\"Royalblue\",\n",
    "            order=table.sort_values(by=\"Precio/mts2\", ascending=False).Barrios, saturation=1)\n",
    "plt.title(\"Valor del metro cuadrado\".upper(), fontsize=20)\n",
    "plt.xlabel(\"Barrios\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.xticks(fontsize=15, horizontalalignment=\"center\", rotation=90)\n",
    "plt.ylabel(\"Precio/mts2 promedio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "plt.yticks(fontsize=15, verticalalignment=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "\n",
    "Creamos clusters a partir de barrios para poder generar mejores datos para realizar las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIÓN PARA GRAFICAR LOS CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, db, labels, n_clusters):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    # Armamos una mascara, con unos en los datos que son CORES.\n",
    "    core_samples_mask_1 = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask_1[db.core_sample_indices_] = True\n",
    "    # Plot result\n",
    "\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask & core_samples_mask_1]\n",
    "        plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], \"o\", markerfacecolor=tuple(col),\n",
    "                 markeredgecolor=\"k\", markersize=14)\n",
    "\n",
    "        xy = X[class_member_mask & ~core_samples_mask_1]\n",
    "        plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], \"o\", markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "    plt.title(\"Estimated number of clusters: %d\" % n_clusters, fontsize=20)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#clustering = DBSCAN(eps=2, min_samples=50, n_jobs=-1)\n",
    "#eps=5, min_samples=50,\n",
    "clustering = DBSCAN(n_jobs=-1)\n",
    "clustering.fit_predict(kmeans)\n",
    "# Consultamos las etiquetas\n",
    "label = clustering.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters = len(set(label)) - (1 if -1 in label else 0)\n",
    "n_noise = list(label).count(-1)\n",
    "print(\"Cantidad estimado de cluster: %d\" % n_clusters)\n",
    "print(\"Puntos de ruido estimados: %d\" % n_noise)\n",
    "plot_clusters(kmeans, clustering, label, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-MEANS\n",
    "\n",
    "A través de la variable creada previamente, precio del mt<sup>2</sup>, agrupamos los datos ```Barrios``` y generamos clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster = data.copy()\n",
    "labels = label_encoder.fit_transform(data_cluster[\"Barrios\"])\n",
    "data_cluster[\"Barrios\"] = labels\n",
    "kmeans = data_cluster[[ \"Barrios\", \"Precio/mts2\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos la **inercia media** *—distancia de cada punto al centroide—* y la **silhouette media** *—medida de cuán similar es un objeto a su propio cúmulo (cohesión) en comparación con otros cúmulos (separación)—*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos las listas\n",
    "lista_distancias_medias = []\n",
    "lista_sil = []\n",
    "# Entrenamos un modelo para cada numero de cluster que queremos testear\n",
    "K = np.arange(2,8)\n",
    "for k in K:    \n",
    "    # Definimos y entrenamos el modelo\n",
    "    km = KMeans(n_clusters=k, random_state=10)\n",
    "    km = km.fit(kmeans)\n",
    "    etiquetas = km.labels_\n",
    "    # Tomamos la suma de las distancias para todas las instancias del dataset\n",
    "    distancia_total = km.inertia_\n",
    "\n",
    "    # Calculamos la distancia media y agregamos a la lista\n",
    "    distancia_media = distancia_total/k\n",
    "    lista_distancias_medias.append(distancia_media)\n",
    "\n",
    "    # Calculamos el silhouette\n",
    "    valor_medio_sil = silhouette_score(kmeans, etiquetas)\n",
    "    lista_sil.append(valor_medio_sil)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5), tight_layout=True)\n",
    "plt.suptitle(f\"ELBOW: Innertia and Silhouette\".upper(), fontsize=15, fontweight=\"semibold\", y=1.05)\n",
    "\n",
    "ax[0].plot(K, lista_distancias_medias, color=\"Royalblue\", linewidth=2)\n",
    "ax[0].scatter(K, lista_distancias_medias, color=\"Coral\", s=70)\n",
    "ax[0].tick_params(labelsize=15)\n",
    "ax[0].set_title(\"Inertia\".upper())\n",
    "ax[0].set_xlabel(\"Cantidad de clusters\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "ax[0].set_ylabel(\"Inercia media\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "\n",
    "ax[1].plot(K, lista_sil, color=\"Royalblue\", linewidth=2)\n",
    "ax[1].scatter(K, lista_sil, color=\"Coral\", s=70)\n",
    "ax[1].tick_params(labelsize=15)\n",
    "ax[1].set_title(\"Silhouette\".upper())\n",
    "ax[1].set_xlabel(\"Cantidad de clusters\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "ax[1].set_ylabel(\"Silhouette media\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aca ponen el Dataset con el que quieren trabajar\n",
    "X_std = kmeans\n",
    "# Aca definen la lista de ks para los cuales quieren hacer un gráfico\n",
    "cantidad_clusters = [ 2, 3, 4, 5 ]\n",
    "for i, k in enumerate(cantidad_clusters):\n",
    "    # Run the Kmeans algorithm\n",
    "    km = KMeans(n_clusters=k, random_state=10)\n",
    "    labels = km.fit_predict(X_std)\n",
    "    centroids = km.cluster_centers_     \n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(X_std, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(15,10), tight_layout=True)\n",
    "    plt.suptitle(f\"Silhouette analysis using K={ k }\".upper(), fontsize=15, fontweight=\"semibold\",\n",
    "                 y=1.025)\n",
    "\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        colors = cm.Paired(float(i)/k)\n",
    "        ax[0].barh(range(y_lower, y_upper), cluster_silhouette_vals, facecolor=colors,\n",
    "                   edgecolor=colors, height=1)\n",
    "        ax[0].text(-0.03, (y_lower + y_upper)/2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax[0].axvline(avg_score, linestyle=\"--\", linewidth=4, color=\"Royalblue\")\n",
    "    ax[0].set_title(\"Silhouette plot for the various clusters\".upper(), y=1.02)\n",
    "    ax[0].set_xlabel(\"Silhouette coefficient values\".upper(), fontsize=10, fontweight=\"bold\",\n",
    "                      labelpad=15)\n",
    "    ax[0].set_ylabel(\"Etiquetas\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "    ax[0].set_ylim([0, len(kmeans) + (k + 1) * 10])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].tick_params(labelsize=15)\n",
    "    \n",
    "    # Scatter plot of data colored with labels\n",
    "    colors = cm.Paired(labels.astype(float)/k)\n",
    "    ax[1].scatter(data[\"Barrios\"], data[\"Precio/mts2\"], alpha=.8, color=colors)\n",
    "    ax[1].scatter(centroids[:, 0], centroids[:, 1], marker=\"+\", color=\"black\", s=500)\n",
    "    ax[1].set_title(\"Visualization of clustered data\".upper(), y=1.02)\n",
    "    ax[1].set_xlabel(\"Barrios\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "    ax[1].set_ylabel(\"Precio/mts2\".upper(), fontsize=10, fontweight=\"bold\", labelpad=15)\n",
    "    ax[1].tick_params(axis=\"x\", rotation=90)\n",
    "    ax[1].tick_params(labelsize=15)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando como base el análisis previo de **ELBOW** y **SILHOUETTE**, entrenamos un modelo de ```KMeans``` buscando identificar cuatro clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=4, max_iter=500, random_state=10)\n",
    "k_means.fit(kmeans)\n",
    "etiquetas = k_means.labels_\n",
    "centros = k_means.cluster_centers_\n",
    "data_cluster[\"Clusters\"] = etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x='Barrios', y='Precio/mts2', data=data_cluster, hue='Clusters', palette='Dark2')\n",
    "sns.scatterplot(x=centros[:, 0], y=centros[:, 1], color=\"Black\", linewidth=2, marker=\"+\", s=500)\n",
    "plt.legend(fontsize=12, title='CLUSTERS', title_fontsize=14)\n",
    "plt.title(\"Clusterización barrios\".upper(), fontsize=15)\n",
    "plt.xlabel('Barrios'.upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.xticks(fontsize=15, rotation=90)\n",
    "plt.ylabel(\"Precio/mts2\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x='Sup. total', y='Precio', data=data_cluster, hue='Clusters', palette='Dark2')\n",
    "plt.legend(fontsize=12, title='CLUSTERS', title_fontsize=14)\n",
    "plt.title(\"Análisis de Sup. total con precio\".upper(), fontsize=15)\n",
    "plt.xlabel(\"Sup. total\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Precio\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x='Sup. total', y='Precio/mts2', data=data_cluster, hue='Clusters', palette='Dark2')\n",
    "plt.legend(fontsize=12, title='CLUSTERS', title_fontsize=14)\n",
    "plt.title(\"Análisis de Sup. total con Precio/mts2\".upper(), fontsize=15)\n",
    "plt.xlabel(\"Sup. total\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Precio/mts2\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x='Precio', y='Precio/mts2', data=data_cluster, hue='Clusters', palette='Dark2')\n",
    "plt.legend(fontsize=12, title='CLUSTERS', title_fontsize=14)\n",
    "plt.title(\"Análisis del precio con Precio/mts2\".upper(), fontsize=15)\n",
    "plt.xlabel(\"Precio\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Precio/mts2\".upper(), fontsize=10, fontweight=\"bold\", labelpad=10)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    cluster = data_cluster.query(f\"Clusters == { i }\").groupby(by=[\"Barrios\"])\\\n",
    "               .agg(Promedio=pd.NamedAgg(column=\"Precio/mts2\", aggfunc=\"mean\")).reset_index()\n",
    "    plt.figure(figsize=(20,5))\n",
    "    sns.barplot(x=\"Barrios\", y=\"Promedio\", data=cluster, edgecolor=\"black\", linewidth=2, color=\"Royalblue\",\n",
    "                order=cluster.sort_values(\"Promedio\", ascending=False).Barrios.values)\n",
    "    #sns.despine(top=True, right=True)\n",
    "    plt.title(f\"Cluster { i }\".upper(), fontsize=15, weight=\"bold\")\n",
    "    plt.xlabel(\"Barrios\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    plt.xticks(fontsize=15, rotation=90)\n",
    "    plt.ylabel(\"Precio/mt2 promedio\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding = one_hot_encoder.fit_transform(data_cluster[[\"Clusters\"]])\n",
    "columns = pd.DataFrame(encoding, columns=['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4'])\n",
    "data_cluster = pd.concat([data_cluster, columns], axis=1)\n",
    "data_cluster[\"Cluster 1\"] = data_cluster[\"Cluster 1\"].astype(\"int32\")\n",
    "data_cluster[\"Cluster 2\"] = data_cluster[\"Cluster 2\"].astype(\"int32\")\n",
    "data_cluster[\"Cluster 3\"] = data_cluster[\"Cluster 3\"].astype(\"int32\")\n",
    "data_cluster[\"Cluster 4\"] = data_cluster[\"Cluster 4\"].astype(\"int32\")\n",
    "data_cluster.drop(columns=['Clusters'], inplace=True)\n",
    "#labels = label_encoder.fit_transform(data_cluster[\"Barrios\"])\n",
    "#data_cluster['Barrios'] = labels\n",
    "data_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de datos\n",
    "\n",
    "Realizamos un escalado de los atributos ```Ambientes```, ```Baños```, ```Habitaciones```, ```Sup. cubierta``` y ```Sup. total``` utilizando ```MinMaxScaler```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escalado = data_cluster.copy()\n",
    "min_max = MinMaxScaler()\n",
    "X = escalado[[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\", \"Precio/mts2\" ]]\n",
    "values = min_max.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluimos los atributos categóricos en los datos escalados y creamos la variable ```X```. Establecemos la variable, ```y```, que vamos a predecir, con el atributo ```precio```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(values, columns=[ \"Ambientes\", \"Baños\", \"Habitaciones\", \"Sup. cubierta\", \"Sup. total\", \"Precio/mts2\" ])\n",
    "X[\"Casa\"] = data_cluster[\"Casa\"]\n",
    "X[\"Dpto\"] = data_cluster[\"Dpto\"]\n",
    "X[\"PH\"] = data_cluster[\"PH\"]\n",
    "X[\"Barrios\"] = data_cluster[\"Barrios\"]\n",
    "X[\"Cluster 1\"] = data_cluster[\"Cluster 1\"]\n",
    "X[\"Cluster 2\"] = data_cluster[\"Cluster 2\"]\n",
    "X[\"Cluster 3\"] = data_cluster[\"Cluster 3\"]\n",
    "X[\"Cluster 4\"] = data_cluster[\"Cluster 4\"]\n",
    "X[\"Precio\"] = data_cluster[\"Precio\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducción de dimensionalidad\n",
    "\n",
    "Vamos a realizar la reducción de dimensionalidad utilizando ```PCA``` y ```SVD```.\n",
    "\n",
    "Creamos una función para analizar la mejor reducción de dimensionalidad posible analizando la *varianza* y la *raíz del error cuadrático medio*, el *error cuadrático medio* y el *coeficiente de determinación*. Para este análisis utilizamos la validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(x_values, y_values, size, regressor):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=size, random_state=5)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_predict_train = regressor.predict(X_train)\n",
    "    y_predict_test = regressor.predict(X_test)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_predict_train))\n",
    "    round(rmse_train, 2)\n",
    "    mean_squared_error(y_train, y_predict_train).mean().round(2)\n",
    "    r2_score(y_train, y_predict_train).mean().round(2)\n",
    "    \n",
    "    \n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_predict_test))\n",
    "    round(rmse_test, 2)\n",
    "    mean_squared_error(y_test, y_predict_test).mean().round(2)\n",
    "    r2_score(y_test, y_predict_test).mean().round(2)\n",
    "\n",
    "\n",
    "def reduce_data(reducer, x_values, y_values, components=[2,3,4,5], regressor=LinearRegression(n_jobs=-1)):\n",
    "    mse, rmse, r2 = [], [], []\n",
    "    variances = []\n",
    "    for component in components:\n",
    "        reducer.set_params(n_components=component)\n",
    "        values = reducer.fit_transform(x_values)\n",
    "        variances.append(reducer.explained_variance_ratio_.sum())\n",
    "        \n",
    "        rmse_values = cross_val_score(regressor, values, y_values, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "        rmse.append(-rmse_values.mean().round(2))\n",
    "        \n",
    "        mse_values = cross_val_score(regressor, values, y_values, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "        mse.append(-mse_values.mean().round(2))\n",
    "        \n",
    "        r2.append(cross_val_score(regressor, values, y_values, cv=5, scoring=\"r2\", n_jobs=-1).mean().round(2))\n",
    "    return rmse, mse, r2, variances\n",
    "\n",
    "def graficar_reducer(components, rmse, mse, r2, variances, pca=True):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20,6), frameon=True, tight_layout=True)\n",
    "    fig.suptitle(f'Reducción de dimensionalidad: { \"PCA\" if pca else \"SVD\" }', y=1.025, fontsize=20, fontweight='bold')\n",
    "\n",
    "    sns.scatterplot(x=components, y=rmse, alpha=.8, color='Royalblue', s=80, ax=axs[0])\n",
    "    axs[0].set_title(f'RMSE'.upper(), fontsize=15)\n",
    "    axs[0].set_xlabel(f'N° de componentes'.upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[0].set_ylabel(\"Error RMSE\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[0].tick_params(axis=\"both\", labelsize=15)\n",
    "    #axs[0].set_xlim(0,300)\n",
    "\n",
    "    sns.scatterplot(x=components, y=variances, alpha=.8, color=\"Coral\", s=80, ax=axs[1])\n",
    "    axs[1].set_title(f\"Varianza\".upper(), fontsize=15)\n",
    "    axs[1].set_xlabel(f\"N° de componentes\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[1].set_ylabel(\"Varianza\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[1].tick_params(axis=\"both\", labelsize=15)\n",
    "    \n",
    "    sns.scatterplot(x=components, y=r2, alpha=.8, color=\"Royalblue\", s=80, ax=axs[2])\n",
    "    axs[2].set_title(f\"R2\".upper(), fontsize=15)\n",
    "    axs[2].set_xlabel(f\"N° de componentes\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[2].set_ylabel(\"Coeficiente de determinación\".upper(), fontsize=10, labelpad=15, weight=\"bold\")\n",
    "    axs[2].tick_params(axis=\"both\", labelsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un modelo **DecisionTreeRegressor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "regressor = DecisionTreeRegressor(random_state=5)\n",
    "rmse, mse, r2, variances = reduccion_data(PCA(random_state=5), X.drop(columns=['Precio/mts2', 'Precio']), X[['Precio']], \n",
    "                                          components, regressor)\n",
    "graficar_reducer(components, rmse, mse, r2, variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un modelo **KNeighborsRegressor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "regressor = KNeighborsRegressor(n_jobs=-1)\n",
    "rmse, mse, r2, variances = reduccion_data(PCA(random_state=5), X.drop(columns=['Precio/mts2', 'Precio']), X[['Precio']],\n",
    "                                          components, regressor)\n",
    "graficar_reducer(components, rmse, mse, r2, variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, el modelo que mejor desempeño tiene es *DecissionTreeRegressor*. Respecto a la reducción de dimensionalidad de los datos, aplicado PCA de 8 a 10 componentes, obtenemos el menor error posible, a su vez, a partir de ocho componentes, la varianza es la mínima posible.\n",
    "\n",
    "Se probó el método **SVD** para reducir el número de componentes pero su error fue mayor al que se consigue con **PCA**. No se muestran las gráficas para evitar agregar peso adicional al notebook. De todos modos, la función ```reduccion_data``` está adaptada en caso de querer comprobar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "PCA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8, random_state=42)\n",
    "values = pca.fit_transform(X.drop(columns=['Precio/mts2', 'Precio']))\n",
    "pca_data = pd.DataFrame(values, columns=[ \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\" ])\n",
    "pca_data[\"Precio\"] = X[\"Precio\"]\n",
    "\n",
    "valor_x_pca = pca_data.drop(columns=[\"Precio\"])\n",
    "valor_y_pca = pca_data[[\"Precio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [2, 4, 6, 8, 10, 12, 15, 20]\n",
    "sizes = [0.2, 0.3, 0.5, 0.7, 0.8, 1]\n",
    "plt.figure(figsize=(20,6), frameon=True, tight_layout=True, dpi=100)\n",
    "train_scores, test_scores = validation_curve(estimator=DecisionTreeRegressor(random_state=5),\n",
    "                                             X=valor_x_pca, y=valor_y_pca, n_jobs=-1,\n",
    "                                             param_name=\"max_depth\", param_range=depths,\n",
    "                                             scoring=\"neg_root_mean_squared_error\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(depths, pd.Series(-test_scores.mean(axis=1), index=depths), color=\"Coral\",\n",
    "         label=\"Test\", linewidth=2)\n",
    "plt.plot(depths, pd.Series(-train_scores.mean(axis=1), index=depths), color=\"Royalblue\",\n",
    "         label=\"Train\", linewidth=2)\n",
    "plt.title(\"CURVA DE VALIDACIÓN\", fontsize=15)\n",
    "plt.legend(fancybox=True, fontsize=15, frameon=True, loc=\"upper right\")\n",
    "plt.xlabel(\"PROFUNDIDAD\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"RAÍZ DEL ERROR CUADRÁTICO MEDIO\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "train_scores, test_scores, validation_scores = learning_curve(estimator=DecisionTreeRegressor(random_state=5),\n",
    "                                                              X=valor_x_pca, y=valor_y_pca, n_jobs=-1,\n",
    "                                                              random_state=5,\n",
    "                                                              scoring=\"neg_root_mean_squared_error\",\n",
    "                                                              shuffle=True, train_sizes=sizes)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(sizes, pd.Series(-test_scores.mean(axis=1), index=sizes), color=\"Coral\", label=\"Test\",\n",
    "         linewidth=2)\n",
    "plt.plot(sizes, pd.Series(train_scores, index=sizes), color=\"Royalblue\", label=\"Train\", linewidth=2)\n",
    "plt.plot(sizes, pd.Series(-validation_scores.mean(axis=1), index=sizes), color=\"Green\",\n",
    "         label=\"Validación\", linewidth=2)\n",
    "plt.title(\"CURVA DE APRENDIZAJE\", fontsize=15)\n",
    "plt.legend(fancybox=True, fontsize=15, frameon=True, loc=\"upper left\")\n",
    "plt.xlabel(\"PORCENTAJE DE MUESTRAS\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"RAÍZ DEL ERROR CUADRÁTICO MEDIO\", fontsize=10, labelpad=10, weight=\"bold\")\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo del Proyecto 01\n",
    "\n",
    "Entrenamos un modelo, con los mismos hiperparámetros que utilizamos en el Proyecto 01, utilizando los datos del procesamiento que hicimos. Evaluamos su desempeño y realizamos una comparación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIÓN PARA EVALUAR LA REGRESIÓN LINEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_regresion(model, X_train, X_test, y_train, y_test):\n",
    "    y_predict_train = model.predict(X_train)\n",
    "    y_predict_test = model.predict(X_test)\n",
    "    \n",
    "    ### CALCULAMOS LOS ERRORES\n",
    "    print(\"=========================================\")\n",
    "    print(\"TRAIN:\")\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_predict_train))\n",
    "    print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { round(rmse_train, 2) }\")\n",
    "    print(f\"ERROR CUADRÁTICO MEDIO: { mean_squared_error(y_train, y_predict_train).mean().round(2) }\")\n",
    "    print(f\"COEFICIENTE DE DETERMINACIÓN: { r2_score(y_train, y_predict_train).mean().round(2) }\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"TEST:\")\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_predict_test))\n",
    "    print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { round(rmse_test, 2) }\")\n",
    "    print(f\"ERROR CUADRÁTICO MEDIO: { mean_squared_error(y_test, y_predict_test).mean().round(2) }\")\n",
    "    print(f\"COEFICIENTE DE DETERMINACIÓN: { r2_score(y_test, y_predict_test).mean().round(2) }\")\n",
    "    print(\"=========================================\")\n",
    "    \n",
    "    ### GRAFICAMOS LOS RESULTADOS\n",
    "    plt.figure(figsize=(20,8), tight_layout=True)\n",
    "    plt.subplot(1,2,1)\n",
    "    x = (y_train - y_predict_train.reshape(-1,1))\n",
    "    plt.hist(x.values, align=\"mid\", bins=100, color=\"Royalblue\", density=True,\n",
    "             label=\"Train\", stacked=True, linewidth=2, edgecolor=\"white\")\n",
    "    x = (y_test - y_predict_test.reshape(-1,1))\n",
    "    plt.hist(x.values, align=\"mid\", alpha=1, bins=100, color=\"Coral\", density=True,\n",
    "             label=\"Test\", stacked=True, histtype=\"step\", linewidth=4)\n",
    "    plt.legend(fontsize=25, fancybox=True, frameon=True)\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.xlabel(\"Errores\".upper(), fontsize=15, fontweight=\"bold\", labelpad=15)\n",
    "    #plt.xlim(-100000,100000)\n",
    "    plt.ylabel(\"Probabilidad\".upper(), fontsize=15, fontweight=\"bold\", labelpad=15)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(y_train, y_predict_train, alpha=.8, color=\"Royalblue\", label=\"Train\", marker=\"o\", s=50)\n",
    "    plt.scatter(y_test, y_predict_test, alpha=.4, color=\"Coral\", label=\"Test\",  marker=\"*\", s=15)\n",
    "    lims = [\n",
    "        np.min([plt.xlim(), plt.ylim()]),  # min of both axes\n",
    "        np.max([plt.xlim(), plt.ylim()]),  # max of both axes]\n",
    "    ]\n",
    "    plt.plot(lims, lims, color=\"black\", linewidth=2, zorder=0)\n",
    "    plt.legend(fontsize=25, fancybox=True, frameon=True)\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.xlabel(\"Y\".upper(), fontsize=15, fontweight=\"bold\", labelpad=15)\n",
    "    plt.ylabel(\"Predicción\".upper(), fontsize=15, fontweight=\"bold\", labelpad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DecisionTreeRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(random_state=5)\n",
    "print(\"DecisionTreeRegressor PCA\")\n",
    "print(\"-----------------------------------------\")\n",
    "rmse = cross_val_score(regressor, valor_x_pca, valor_y_pca, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { -rmse.mean().round(2) }\")\n",
    "mse = cross_val_score(regressor, valor_x_pca, valor_y_pca, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"ERROR CUADRÁTICO MEDIO:: { -mse.mean().round(2) }\")\n",
    "r2 = cross_val_score(regressor, valor_x_pca, valor_y_pca, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "print(f\"COEFICIENTE DE DETERMINACIÓN: { r2.mean().round(2) }\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(valor_x_pca, valor_y_pca, test_size=0.5, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "evaluar_regresion(regressor, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo una comparación de resultados entre el modelo del proyecto uno, y, utilizando el mismo modelo de machine learning pero con el pre-procesamiento de datos logrados y llevados a cabo en este proyecto, conseguimos una mejora notable.\n",
    "El agregado de nuevos datos en el análisis, como la codificación de variables categóricas, la detección de clusters, a si mismo, como el tratamiento que se realizó con los mismo, con esto nos referimos al tratamiento de datos atípicos, el escalado de los mismos, la imputación de valores faltantes, logramos una reducción del error del 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE B: Modelos avanzados**\n",
    "\n",
    "Empezamos esta etapa realizan un ```train_test_split``` de los datos. Utilizamos el modelo ```learning_curve``` de la librería ```model_selection``` para elegir la mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegressor\n",
    "\n",
    "Instanciamos el modelo de regresión lineal y utilizando la curva de validación elegimos la mejor proporción de datos que utilizamos para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------\")\n",
    "print(\"LinearRegression\")\n",
    "print(\"-----------------------------------------\")\n",
    "rmse = cross_val_score(LinearRegression(n_jobs=-1), valor_x_pca, valor_y_pca, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { -rmse.mean().round(2) }\")\n",
    "mse = cross_val_score(LinearRegression(n_jobs=-1), valor_x_pca, valor_y_pca, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"ERROR CUADRÁTICO MEDIO:: { -mse.mean().round(2) }\")\n",
    "r2 = cross_val_score(LinearRegression(n_jobs=-1), valor_x_pca, valor_y_pca, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "print(f\"COEFICIENTE DE DETERMINACIÓN: { r2.mean().round(2) }\")\n",
    "\n",
    "evaluar_regresion(LinearRegression(n_jobs=-1).fit(X_train, y_train),\n",
    "                  X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynominal features\n",
    "\n",
    "Generamos nuevas variables con un modelo ```PolynomialFeatures``` y, a través de la validación cruzada, evaluamos la regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_polynomial = polynomial.fit_transform(valor_x_pca)\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"PolynomialFeatures\")\n",
    "print(\"------------------------------------------\")\n",
    "rmse = cross_val_score(LinearRegression(n_jobs=-1), X_polynomial, valor_y_pca, cv=5,\n",
    "                       scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { -rmse.mean().round(2) }\")\n",
    "mse = cross_val_score(LinearRegression(n_jobs=-1), X_polynomial, valor_y_pca, cv=5,\n",
    "                      scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"ERROR ABSOLUTO MEDIO: { -mse.mean().round(2) }\")\n",
    "r2 = cross_val_score(LinearRegression(n_jobs=-1), X_polynomial, valor_y_pca, cv=5,\n",
    "                     scoring=\"r2\", n_jobs=-1).mean().round(2)\n",
    "print(f\"COEFICIENTE DE DETERMINACIÓN: { r2 }\")\n",
    "evaluar_regresion(LinearRegression(n_jobs=-1).fit(X_train_polynomial_pca, y_test),\n",
    "                  X_train_polynomial_pca, X_test_polynomial_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularización\n",
    "\n",
    "---\n",
    "\n",
    "RIDGE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=20)\n",
    "ridge.fit(X_train_polynomial_pca, y_train)\n",
    "evaluar_regresion(ridge, X_train_polynomial_pca, X_test_polynomial_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = [ 0.005, 0.01, 0.1, 0.5, 1, 5 ]\n",
    "for i in alpha:\n",
    "    ridge = Ridge(alpha=i, max_iter=200, random_state=20)\n",
    "    ridge.fit(X_train_polynomial_pca, y_train)\n",
    "    print(f\"RIDGE (alpha: { i })\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(f\"Pendientes: { ridge.coef_ }\")\n",
    "    print(f\"Ordenada: { ridge.intercept_ }\")\n",
    "    evaluar_regresion(ridge, X_train_polynomial_pca, X_test_polynomial_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "LASSO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(random_state=20)\n",
    "lasso.fit(X_train_polynomial_pca, y_train)\n",
    "evaluar_regresion(lasso, X_train_polynomial_pca, X_test_polynomial_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = [ 0.005, 0.01, 0.1, 0.5, 1, 5 ]\n",
    "for i in alpha:\n",
    "    lasso = Lasso(alpha=i, max_iter=200, random_state=20)\n",
    "    lasso.fit(X_train_polynomial_pca, y_train)\n",
    "    print(f\"LASSO (alpha: { i })\")\n",
    "    print(f\"Pendientes: { lasso.coef_ }\")\n",
    "    print(f\"Ordenada: { lasso.intercept_ }\")\n",
    "    evaluar_regresion(lasso, X_train_polynomial_pca, X_test_polynomial_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "polynomial = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_polynomial = polynomial.fit_transform(valor_x_svd)\n",
    "print(\"LinearRegression SVD\")\n",
    "rmse = cross_val_score(regressor, X_polynomial, valor_y_svd, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { -rmse.mean().round(2) }\")\n",
    "mse = cross_val_score(regressor, X_polynomial, valor_y_svd, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "print(f\"ERROR ABSOLUTO MEDIO: { -mse.mean().round(2) }\")\n",
    "r2 = cross_val_score(regressor, X_polynomial, valor_y_svd, cv=5, scoring=\"r2\", n_jobs=-1).mean().round(2)\n",
    "print(f\"COEFICIENTE DE DETERMINACIÓN: { r2 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(valor_x_svd, valor_y_svd, test_size=0.7, random_state=42)\n",
    "polynomial = PolynomialFeatures(degree=8, include_bias=False)\n",
    "X_train_polynomial_svd = polynomial.fit_transform(X_train)\n",
    "X_test_polynomial_svd = polynomial.fit_transform(X_test)\n",
    "regressor.fit(X_train_polynomial_svd, y_train)\n",
    "\n",
    "y_predict_train = regressor.predict(X_train_polynomial_svd)\n",
    "print(\"TRAIN:\")\n",
    "print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { mean_absolute_error(y_train, y_predict_train).round(2) }\")\n",
    "print(f\"ERROR ABSOLUTO MEDIO: { mean_absolute_error(y_train, y_predict_train).round(2) }\")\n",
    "print(f\"COEFICIENTE DE DETERMINACIÓN: { r2_score(y_train, y_predict_train).round(2) }\")\n",
    "y_predict_test = regressor.predict(X_test_polynomial_svd)\n",
    "print(\"\\nTEST:\")\n",
    "print(f\"RAÍZ DEL ERROR CUADRÁTICO MEDIO: { mean_absolute_error(y_test, y_predict_test).round(2) }\")\n",
    "print(f\"ERROR ABSOLUTO MEDIO: { mean_absolute_error(y_test, y_predict_test).round(2) }\")\n",
    "print(f\"COEFICIENTE DE DETERMINACIÓN: { r2_score(y_test, y_predict_test).round(2) }\\n\")\n",
    "evaluar_regresion(regressor, X_train_polynomial_svd, X_test_polynomial_svd, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Regularización\n",
    "\n",
    "---\n",
    "\n",
    "RIDGE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = [ 0.005, 0.01, 0.1, 0.5, 1, 5 ]\n",
    "for i in alpha:\n",
    "    ridge = Ridge(alpha=i, max_iter=200, random_state=20)\n",
    "    ridge.fit(X_train_polynomial_svd, y_train)\n",
    "    print(f\"RIDGE (alpha: { i })\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(f\"Pendientes: { ridge.coef_ }\")\n",
    "    print(f\"Ordenada: { ridge.intercept_ }\")\n",
    "    evaluar_regresion(ridge, X_train_polynomial_svd, X_test_polynomial_svd, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "LASSO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = [ 0.005, 0.01, 0.1, 0.5, 1, 5 ]\n",
    "for i in alpha:\n",
    "    lasso = Lasso(alpha=i, max_iter=200, random_state=20)\n",
    "    lasso.fit(X_train_polynomial_svd, y_train)\n",
    "    print(f\"LASSO (alpha: { i })\")\n",
    "    print(f\"Pendientes: { lasso.coef_ }\")\n",
    "    print(f\"Ordenada: { lasso.intercept_ }\")\n",
    "    evaluar_regresion(lasso, X_train_polynomial_svd, X_test_polynomial_svd, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE C: Interpretación de los modelos**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "484.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}